# 🚀 Explainable AI (XAI)
Explainable AI (XAI) refers to a set of techniques, methods, and frameworks aimed at making the decision-making processes of artificial intelligence (AI) systems transparent, interpretable, and understandable to humans. Unlike traditional AI models, which often operate as “black boxes,” XAI provides insights into how and why AI systems make specific predictions or decisions. This is achieved through visualizations, feature attributions, or other forms of explanation that highlight the relationships between inputs and outputs in the model.

![img](https://github.com/user-attachments/assets/bfb21683-41d9-40b4-9bd4-1c0327ea56a9)
<p align="center">Image created by the author using a generative AI tool</p>


# 📌 Why We Need Explainability in AI?
Modern AI systems, particularly those powered by deep learning, rely on intricate architectures involving millions of parameters. These systems often provide astonishingly accurate predictions but fail to justify their decisions in human terms. For instance, a deep learning model diagnosing a medical condition might output a high probability of disease, but without an explanation, medical professionals cannot trust or validate the diagnosis. This lack of interpretability undermines users’ confidence in AI and may lead to resistance to adoption.

The need for explainability extends beyond trust. Regulations like the European Union’s General Data Protection Regulation (GDPR) mandate the “right to explanation,” requiring AI systems to provide understandable reasons for decisions, especially when they significantly impact individuals. Explainability also enhances system debugging, facilitating developers’ ability to identify and correct biases or errors in AI models.

# 🌀 Methods for Achieving Explainability
Explainable AI employs various methods to make complex models interpretable. These methods can be broadly categorized into intrinsic and post-hoc approaches.

**Intrinsic Explainability**
Some models are inherently explainable by design. Decision trees, for instance, provide a natural structure where each node represents a decision rule, and each branch explains the outcome of that rule.

Similarly, linear regression models offer coefficients that directly quantify the impact of each feature on the prediction. However, these methods often sacrifice predictive power for interpretability, making them less suitable for high-dimensional, non-linear problems.

**Post-Hoc Explainability**
For complex models like deep neural networks, post-hoc explainability methods are applied after training to interpret their predictions. One popular approach is Local Interpretable Model-agnostic Explanations (LIME). LIME approximates the behavior of an opaque model locally around a specific prediction by fitting an interpretable model, such as a linear regression, to mimic its output. This allows users to understand the model’s reasoning for individual predictions.

Another widely used technique is SHAP (Shapley Additive Explanations). SHAP assigns each feature a contribution score based on game-theoretic principles, offering insights into how each input influenced the prediction. For example, in a loan approval system, SHAP values can reveal whether an applicant’s income, credit score, or other factors led to the decision.

# SHAP (Shapley Additive Explanations)
**Pseudocode:**
 1: procedure SHAP(f, x, F)
 2:     φ ← [0] for all features in F
 3:     for each feature i in F do
 4:         for each subset S ⊆ (F \ {i}) do
 5:             Δ ← f(S ∪ {i}) - f(S)
 6:             weight ← |S| / (|F| choose |S|)
 7:             φ[i] ← φ[i] + weight * Δ
 8:         end for
 9:     end for
10:    return φ
11: end procedure

![download](https://github.com/user-attachments/assets/f821dcaf-e8a6-48d2-8920-b7b65dcdd9b3)

# LIME vs. SHAP
![vs](https://github.com/user-attachments/assets/6dc8808d-796d-49d5-998b-a37244e90fe5)

# 📚 Read More About PINNs at: 
🌐 https://medium.com/tech-spectrum/what-is-explainable-ai-xai-how-does-it-work-e5801d5c8da8
